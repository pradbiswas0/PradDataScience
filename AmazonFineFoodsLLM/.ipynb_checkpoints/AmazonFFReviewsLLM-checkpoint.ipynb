{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "804b1e30-2b99-41e4-8799-f004ef58c7d4",
   "metadata": {},
   "source": [
    "## Large language model project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b719c7-1e61-45e9-87f6-f248ea034ebd",
   "metadata": {},
   "source": [
    "### The dataset (source: Kaggle.com)\n",
    "#### This dataset consists of reviews of Fine Foods from amazon. The data span a period of more than 10 years, including all ~500,000 reviews up to October 2012. Reviews include product and user information, ratings, and a plain text review. It also includes reviews from all other Amazon categories.\n",
    "#### Columns: Id,ProductId,UserId,ProfileName,HelpfulnessNumerator,HelpfulnessDenominator,Score,Time,Summary,Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6374e1d4-b345-45ab-aee3-92aad984d87a",
   "metadata": {},
   "source": [
    "### Objective: Here in this project we will try out 2 things:\n",
    "#### 1) Tokenizing the comment text to generate numerical features which can be used to automatically assign ratings to products. This can be helpful in standardizing scores because humans are inherently subjective resulting in a disconnect between what people say in their comments and the scores they give to a product\n",
    "#### 2) Generating short summaries of comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b0a9bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # dataframe manipulation\n",
    "import numpy as np # linear algebra\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65573b56-9da6-437b-a67f-a2d7ced40809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the raw file (for the score) and the text embeddings\n",
    "df_text=pd.read_csv('embedding_train_amazon.csv')\n",
    "df_amazon_raw = pd.read_csv('Reviews.csv')\n",
    "df_score=df_amazon_raw[['Score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3099ed33-c213-4b0b-9ef4-c5c55dd7cf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the score column alongside the text embeddings\n",
    "df_text['Score']=df_amazon_raw[['Score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0ba62a-a3fa-4288-9227-5522082e8148",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd0c98a-288d-449a-a5a4-45a9b7c2bd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomize the rows\n",
    "df_text=df_text.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d323bbd-ac07-4f90-b5ae-ecd2c96efff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score=df_text[['Score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c107993c-2f6c-4afd-a758-e6b7b6234500",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_desc=df_text.drop(['Score'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df61b43d-10a9-4789-b78b-bc44cd4a3d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score_train, y_score_test = train_test_split(df_score, test_size=0.4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ae149e-2a07-432a-84a8-06184768bceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_text_train, x_text_test = train_test_split(df_desc, test_size=0.4, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be35498-ba5b-4b09-9d62-f90d09257b92",
   "metadata": {},
   "source": [
    "#### Looking for an algorithm which executes relatively fast and requires less memory, we are going to try the LGBMClassifier.\n",
    "#### LGBMClassifier stands for Light Gradient Boosting Machine Classifier. It uses decision tree algorithms for ranking, classification, and other machine-learning tasks. LGBMClassifier uses a novel technique of Gradient-based One-Side Sampling (GOSS) and Exclusive Feature Bundling (EFB) to handle large-scale data with accuracy, effectively making it faster and reducing memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d19944e-b841-4fc8-83e0-22a7102ee926",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c3b6f8-10e2-4a99-b8ab-119895fed03f",
   "metadata": {},
   "source": [
    "##### We are first going to try this with default settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420890b4-0371-46b8-b517-d9bae9afeb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_km = lgb.LGBMClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e08026-2fdf-4985-ab00-203d4651827e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf_km.fit(X = x_text_train , y = y_score_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dca2c24-c856-4099-818f-69cd3f5f751f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf_km.predict(x_text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4139b3-644a-40e1-8073-a1159c189ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy=accuracy_score(y_score_test,y_pred)\n",
    "print('Training-set accuracy score: {0:0.4f}'. format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26631f5-4ea8-4c19-b0b2-6b66bc6f85b2",
   "metadata": {},
   "source": [
    "##### Accuracy of 71% is OK but nothing to write home about.\n",
    "##### Let's see if changing some of the hyper parameters will be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab9ffd3-585c-4bca-9d55-8796f6d876e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_km = lgb.LGBMClassifier(n_estimators=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dac36c9-6301-4d77-a0d9-8e726318019d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf_km.fit(X = x_text_train , y = y_score_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e92e59b-4b5b-41cf-9a51-f813e261a065",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_1 = clf_km.predict(x_text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d7ee6a-17ef-43e4-a3b1-ca7694ed111a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy=accuracy_score(y_score_test,y_pred_1)\n",
    "print('Training-set accuracy score: {0:0.4f}'. format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ec1970-5854-483a-8f59-45d38a749f22",
   "metadata": {},
   "source": [
    "##### Accuracy of 77% is OK but still nothing to write home about.\n",
    "##### Let's see if using a different algorithm makes a difference - enter the famous XGBoost !!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a12f1d-6d55-4289-aa83-734e006541a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd00d62f-34cc-4490-a60e-57a07fb420ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_XGB = xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92caebd-4da1-4580-b673-7bb583540dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_score_train = le.fit_transform(y_score_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c3b3cd-67d9-4f01-a1a2-011ab891e94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_XGB.fit(X = x_text_train , y = y_score_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf25ad2-219c-476f-a4ed-5247ef39aea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_2 = clf_XGB.predict(x_text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74262930-ecf2-4460-8f9b-c774f42f2527",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_2=accuracy_score(y_score_test,y_pred_2)\n",
    "print('Training-set accuracy score: {0:0.4f}'. format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981e6a5a-3a51-452f-8d15-1d28c829e6ee",
   "metadata": {},
   "source": [
    "#### Still 77%\n",
    "#### We can of course try other algorithms to see if the accuracy can be improved further, but I am getting the feeling that the accuracy will probably not go much higher. This is because of the inherent subjectivity of humans and the lack of a rigorous relationship between what we write in the description and the score.\n",
    "#### Let's now try the summary real quick to see if it gives better prediction of scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05feff13-dbd7-454a-b19c-3b89ee693dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summ=pd.read_csv(\"embedding_train_amazon_summ.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298f8d53-ba3a-4ad5-a0b9-e4e624520a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summ['Score']=df_amazon_raw[['Score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9733a7-f412-410c-bc72-8aec0e44b3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summ=df_summ.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50dba46-4a2d-4c87-b4b8-9dde90e4f869",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score_summ=df_summ[['Score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ffd4cb-bcb8-415e-9ba6-47c788509747",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text_summ=df_summ.drop(['Score'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be612fb4-b426-49dd-9977-841614dbabb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score_train, y_score_test = train_test_split(df_score_summ, test_size=0.4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d24edd-51a9-435a-ba24-a8129f851e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_text_train, x_text_test = train_test_split(df_text_summ, test_size=0.4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f301ab60-1831-4608-b5a4-7a9c45a8024f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_km = lgb.LGBMClassifier(n_estimators=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583d7b5c-07f3-4233-98ed-63cdf6675a35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf_km.fit(X = x_text_train , y = y_score_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff80979-a196-4323-9e70-6d166a47413e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_1 = clf_km.predict(x_text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e2a326-2ffc-4dd4-a282-790960cfa595",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy=accuracy_score(y_score_test,y_pred_1)\n",
    "print('Training-set accuracy score: {0:0.4f}'. format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ceb00f7-4f6a-4c13-b789-3fafeffec43a",
   "metadata": {},
   "source": [
    "##### Slightly better than the descriptive comments. But we are still up against the inherent subjectivity of the human mind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d94b39-851e-446f-95e6-a6c2f438f2e1",
   "metadata": {},
   "source": [
    "### We will now use a trained LLM to generate summaries of documents\n",
    "### We are looking for a light-weight LLM which can execute on a decent PC.\n",
    "### I tried GPT-J (model size: 6B) and C4AI Command-R (model size: 35B) and my laptop ran out of memory.\n",
    "### I tried Llama 3B v2 but it was quite slow.\n",
    "### So I then tried Mistral 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c522153c-6087-4071-8bad-7919e1951012",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fireworks.client\n",
    "import os\n",
    "import dotenv\n",
    "import chromadb\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "\n",
    "# you can set envs using Colab secrets\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "fireworks.client.api_key = 'KTGKcoCndQttxHOjG4cYALmEXR0ByhYBgtrozJesElA5eJ2A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9a28c52-48e3-4b83-a6bf-d91be1ed99e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=None, max_tokens=50):\n",
    "\n",
    "    fw_model_dir = \"accounts/fireworks/models/\"\n",
    "\n",
    "    if model is None:\n",
    "        model = fw_model_dir + \"llama-v2-7b\"\n",
    "    else:\n",
    "        model = fw_model_dir + model\n",
    "\n",
    "    completion = fireworks.client.Completion.create(\n",
    "        model=model,\n",
    "        prompt=prompt,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82985ae3-48a6-42f5-8c1f-3b9a9c4dae53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_model(textinput):\n",
    "\n",
    "    p1 = \"\"\"[INST] Summarise the following in 15 words :{\"\"\"\n",
    "    p2=\"\"\"}[/INST]\"\"\"\n",
    "    prompt= p1+textinput+p2\n",
    "\n",
    "    summ=get_completion(prompt, model=mistral_llm, max_tokens=200)\n",
    "    return summ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb832a3b-90fa-4b7b-b7de-32fd62d2e063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the LLM\n",
    "mistral_llm = \"mistral-7b-instruct-4k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad6c422e-79e9-44ac-954b-38df4ef90dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text = pd.read_csv('reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e0f6baf-4ded-4a63-b422-f3edae13f7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the first 200 comments\n",
    "df_text_reduced=df_text[['Text']][:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "805161ee-34fc-489f-9a0e-18ed9336c2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = df_text_reduced[['Text']].apply(lambda x: call_model(x.to_json()), axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6de7647c-c717-4ad2-8497-b90b32a27470",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text_reduced=df_text_reduced.assign(summ=summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b9f705a-2069-49a5-bc0d-b17c095d7f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text_reduced.to_csv(\"amazon_summ.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df02965-4453-4d30-b836-cb52ef912cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Commented out - can use this to process in batches when handling large files\n",
    "# Generate embeddings, and index titles in batches\n",
    "#batch_size = 50\n",
    "\n",
    "# loop through batches and generated + store embeddings\n",
    "#for i in tqdm(range(0, len(df_text_red), batch_size)):\n",
    "   # df_text_red_sub=df_text_red[i : i + batch_size]\n",
    " #   summaries = df_text_red_sub[['Text']].apply(lambda x: call_model(x.to_json()), axis=1).tolist()\n",
    "   # df_text_red_sub=df_text_red_sub.assign(summ=summaries)\n",
    " #   filename=\"amazon_summ\"+str(i)+\".csv\"\n",
    "  #  df_text_red_sub.to_csv(filename,index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d648b610-464e-4cdb-b9e1-c39f722ce89a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
